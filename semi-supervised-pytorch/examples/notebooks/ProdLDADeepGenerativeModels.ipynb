{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "cuda = torch.cuda.is_available()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append(\"../../semi-supervised\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import ProdLDADeepGenerativeModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dim = 10\n",
    "z_dim = 32\n",
    "h_dim = [256, 128]\n",
    "\n",
    "num_topics = y_dim\n",
    "a = 1.0\n",
    "prior_mean = np.log(a) - np.mean(np.log(a))\n",
    "prior_var = (((1.0 / a) * (1 - (2.0 / num_topics))) + (1.0 / (num_topics * num_topics)) * np.sum((1.0 / a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ProdLDADeepGenerativeModel([784, y_dim, z_dim, h_dim], prior_mean, prior_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProdLDADeepGenerativeModel(\n",
       "  (encoder): Encoder(\n",
       "    (hidden): ModuleList(\n",
       "      (0): Linear(in_features=794, out_features=256, bias=True)\n",
       "      (1): Linear(in_features=256, out_features=128, bias=True)\n",
       "    )\n",
       "    (sample): GaussianSample(\n",
       "      (mu): Linear(in_features=128, out_features=32, bias=True)\n",
       "      (log_var): Linear(in_features=128, out_features=32, bias=True)\n",
       "    )\n",
       "    (mean_norm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (logvar_norm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (activation): Softmax()\n",
       "    (drop): Dropout(p=0.2)\n",
       "    (hidden): ModuleList(\n",
       "      (0): Linear(in_features=42, out_features=784, bias=True)\n",
       "    )\n",
       "    (norm): BatchNorm1d(784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (output_activation): Softmax()\n",
       "  )\n",
       "  (classifier): Classifier(\n",
       "    (dense): Linear(in_features=784, out_features=256, bias=True)\n",
       "    (logits): Linear(in_features=256, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datautils import get_mnist\n",
    "\n",
    "# Only use 10 labelled examples per class\n",
    "# The rest of the data is unlabelled.\n",
    "labelled, unlabelled, validation = get_mnist(location=\"./\", batch_size=64, labels_per_class=10)\n",
    "alpha = 0.1 * len(unlabelled) / len(labelled)\n",
    "\n",
    "def binary_cross_entropy(r, x):\n",
    "    return -torch.sum(x * torch.log(r + 1e-8) + (1 - x) * torch.log(1 - r + 1e-8), dim=-1)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "from inference import SVI, ImportanceWeightedSampler\n",
    "\n",
    "# You can use importance weighted samples [Burda, 2015] to get a better estimate\n",
    "# on the log-likelihood.\n",
    "sampler = ImportanceWeightedSampler(mc=1, iw=1)\n",
    "\n",
    "if cuda: model = model.cuda()\n",
    "elbo = SVI(model, likelihood=binary_cross_entropy, sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "def train_semi_supervised(labelled, unlabelled, validation, cuda, epochs=4):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss, accuracy = (0, 0)\n",
    "        for (x, y), (u, _) in zip(cycle(labelled), unlabelled):\n",
    "            x, y, u = torch.from_numpy(x).float(), torch.from_numpy(y).float(), torch.from_numpy(u).float()\n",
    "            # Wrap in variables\n",
    "            x, y, u = Variable(x), Variable(y), Variable(u)\n",
    "\n",
    "            if cuda:\n",
    "                # They need to be on the same device and be synchronized.\n",
    "                x, y = x.cuda(device=0), y.cuda(device=0)\n",
    "                u = u.cuda(device=0)\n",
    "\n",
    "            L = -elbo(x, y)\n",
    "            U = -elbo(u)\n",
    "\n",
    "            # Add auxiliary classification loss q(y|x)\n",
    "            logits = model.classify(x)\n",
    "\n",
    "            # Regular cross entropy\n",
    "            classication_loss = torch.sum(y * torch.log(logits + 1e-8), dim=1).mean()\n",
    "\n",
    "            J_alpha = L - alpha * classication_loss + U\n",
    "\n",
    "            J_alpha.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            total_loss += J_alpha.data.item()\n",
    "            accuracy += torch.mean((torch.max(logits, 1)[1].data == torch.max(y, 1)[1].data).float())\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            model.eval()\n",
    "            m = len(unlabelled)\n",
    "            print(\"Epoch: {}\".format(epoch))\n",
    "            print(\"[Train]\\t\\t J_a: {:.2f}, accuracy: {:.2f}\".format(total_loss / m, accuracy / m))\n",
    "\n",
    "            total_loss, accuracy = (0, 0)\n",
    "            for x, y in validation:\n",
    "                x, y = Variable(x), Variable(y)\n",
    "\n",
    "                if cuda:\n",
    "                    x, y = x.cuda(device=0), y.cuda(device=0)\n",
    "\n",
    "                L = -elbo(x, y)\n",
    "                U = -elbo(x)\n",
    "\n",
    "                logits = model.classify(x)\n",
    "                classication_loss = -torch.sum(y * torch.log(logits + 1e-8), dim=1).mean()\n",
    "\n",
    "                J_alpha = L + alpha * classication_loss + U\n",
    "\n",
    "                total_loss += J_alpha.data.item()\n",
    "\n",
    "                _, pred_idx = torch.max(logits, 1)\n",
    "                _, lab_idx = torch.max(y, 1)\n",
    "                accuracy += torch.mean((torch.max(logits, 1)[1].data == torch.max(y, 1)[1].data).float())\n",
    "\n",
    "            m = len(validation)\n",
    "            print(\"[Validation]\\t J_a: {:.2f}, accuracy: {:.2f}\".format(total_loss / m, accuracy / m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate MC samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_samples(num_mc_samples, model, x_batch):\n",
    "    x_batch = torch.from_numpy(x_batch).float()\n",
    "    if cuda:\n",
    "        x_batch = x_batch.cuda()\n",
    "    model.train()\n",
    "    mc_samples_ = [model.classify(x_batch).cpu().detach().numpy() for _ in range(num_mc_samples)]\n",
    "    return np.array(mc_samples_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bald_acq(mc_samples):\n",
    "    expected_entropy = - np.mean(np.sum(mc_samples * np.log(mc_samples + 1e-10), axis=-1), axis=0)  # [batch size]\n",
    "    expected_p = np.mean(mc_samples, axis=0)\n",
    "    entropy_expected_p = - np.sum(expected_p * np.log(expected_p + 1e-10), axis=-1)  # [batch size]\n",
    "    BALD_acq = entropy_expected_p - expected_entropy\n",
    "    \n",
    "    return BALD_acq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_new_data(num_data, num_mc_samples, model, unlabelled_data):\n",
    "    mc_samples_ = mc_samples(num_mc_samples, model, unlabelled_data)\n",
    "    bald_acq_ = bald_acq(mc_samples_)\n",
    "    \n",
    "    return bald_acq_.argsort()[::-1][:num_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_MC_SAMPLES = 100\n",
    "NUM_QUERY = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labelled = list(labelled)\n",
    "train_unlabelled = list(unlabelled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = datasets.MNIST(root=\".\", train=True, download=True, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Split: train\n",
       "    Root Location: .\n",
       "    Transforms (if any): None\n",
       "    Target Transforms (if any): None"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train_data = mnist_train.train_data.reshape(60000, -1).numpy().astype(np.float64)\n",
    "mnist_train_label = mnist_train.train_labels.numpy().astype(np.int64)\n",
    "one_hot_label = np.zeros([mnist_train_label.shape[0], 10])\n",
    "one_hot_label[np.arange(mnist_train_label.shape[0]), mnist_train_label] = 1\n",
    "mnist_train_label = one_hot_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (60000, 10))"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_train_data.shape, mnist_train_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labelled = 300\n",
    "labelled_idx = np.random.choice(mnist_train_data.shape[0], num_labelled, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.zeros(mnist_train_data.shape[0])\n",
    "mask[labelled_idx] = 1\n",
    "mask = mask.astype(np.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled = mnist_train_data[mask], mnist_train_label[mask]\n",
    "unlabelled = mnist_train_data[~mask], mnist_train_label[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch(data, batch_size=64):\n",
    "    x, y = data\n",
    "    batch_idx = np.random.choice(x.shape[0], batch_size, replace=False)\n",
    "\n",
    "    return x[batch_idx], y[batch_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_sets(labelled, unlabelled, batch_size):\n",
    "    num_labelled = labelled[0].shape[0]\n",
    "    num_unlabelled = unlabelled[0].shape[0]\n",
    "\n",
    "    train_labelled = [create_batch(labelled, batch_size) for _ in range(num_labelled // batch_size)]\n",
    "    train_unlabelled = [create_batch(labelled, batch_size) for _ in range(num_unlabelled // batch_size)]\n",
    "    \n",
    "    return train_labelled, train_unlabelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rearange_datasets(labelled, unlabelled, new_data):\n",
    "    labelled_x, labelled_y = labelled\n",
    "    unlabelled_x, unlabelled_y = unlabelled\n",
    "    \n",
    "    new_data_x, new_data_y = unlabelled_x[new_data], unlabelled_y[new_data]\n",
    "    \n",
    "    new_labelled_x = np.append(labelled_x, new_data_x, axis=0)\n",
    "    new_labelled_y = np.append(labelled_y, new_data_y, axis=0)\n",
    "    \n",
    "    new_unlabelled_x = np.delete(unlabelled_x, new_data, axis=0)\n",
    "    new_unlabelled_y = np.delete(unlabelled_y, new_data, axis=0)\n",
    "    \n",
    "    return (new_labelled_x, new_labelled_y), (new_unlabelled_x, new_unlabelled_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310 59690\n",
      "Epoch: 0\n",
      "[Train]\t\t J_a: 302192.51, accuracy: 0.62\n",
      "[Validation]\t J_a: 1422.20, accuracy: 0.60\n",
      "Epoch: 1\n",
      "[Train]\t\t J_a: 302083.98, accuracy: 0.62\n",
      "[Validation]\t J_a: 1420.13, accuracy: 0.60\n",
      "Epoch: 2\n",
      "[Train]\t\t J_a: 301933.31, accuracy: 0.62\n",
      "[Validation]\t J_a: 1419.01, accuracy: 0.60\n",
      "Epoch: 3\n",
      "[Train]\t\t J_a: 301906.13, accuracy: 0.62\n",
      "[Validation]\t J_a: 1422.71, accuracy: 0.60\n",
      "370 59630\n",
      "Epoch: 0\n",
      "[Train]\t\t J_a: 298963.00, accuracy: 0.65\n",
      "[Validation]\t J_a: 1416.88, accuracy: 0.62\n",
      "Epoch: 1\n",
      "[Train]\t\t J_a: 298923.97, accuracy: 0.66\n",
      "[Validation]\t J_a: 1416.59, accuracy: 0.62\n",
      "Epoch: 2\n",
      "[Train]\t\t J_a: 298921.29, accuracy: 0.66\n",
      "[Validation]\t J_a: 1416.11, accuracy: 0.61\n",
      "Epoch: 3\n",
      "[Train]\t\t J_a: 298905.61, accuracy: 0.66\n",
      "[Validation]\t J_a: 1415.63, accuracy: 0.62\n"
     ]
    }
   ],
   "source": [
    "batch_size=60\n",
    "for i in range(2):\n",
    "    train_labelled, train_unlabelled = create_data_sets(labelled, unlabelled, batch_size)\n",
    "    print(labelled[0].shape[0], unlabelled[0].shape[0])\n",
    "    train_semi_supervised(train_labelled, train_unlabelled, validation, cuda, epochs=4)\n",
    "\n",
    "    new_data = query_new_data(NUM_QUERY, NUM_MC_SAMPLES, model, unlabelled[0])\n",
    "    labelled, unlabelled = rearange_datasets(labelled, unlabelled, new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
